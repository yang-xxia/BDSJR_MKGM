# ğŸ—ï¸ BDSJR_MKGM

**Multimodal Knowledge Guided Model (MKGM)** for Joint Recognition of Bridge Defects and Structural Information.

## ğŸ” Overview

Structural defect recognition is a crucial task in bridge inspection. However, existing methods typically focus solely on detecting surface defects while neglecting the structural context such as regions and components present in the background. This limitation hinders fine-grained defect understanding and leads to inaccurate defect attribution.
To address this challenge, **MKGM** introduces a multimodal knowledge-guided framework that jointly recognizes bridge defects along with their associated structural regions and components. It leverages image-text knowledge, concept graphs, and label co-occurrence patterns to guide feature propagation and semantic reasoning across hierarchical labels. The model incorporates the following components:

- ğŸ“· **MambaVision** as the backbone network to extract both global visual features from inspection images  
- ğŸŒ **Multimodal knowledge graphs**, integrating visual object detection, label co-occurrence statistics, and text-based knowledge graph embeddings  
- ğŸ” **Dual-channel GCNs** to propagate features and model hierarchical dependencies across structural and defect labels  
- ğŸ§  **Attention-based fusion** to adaptively integrate multimodal semantic information for robust joint prediction  

---

## ğŸ–¼ï¸ Graphical Abstract

<p align="center">
  <img src="assets/graphical_abstract.png" width="700">
</p>

---

## ğŸ”§ Environment Configuration

This project was developed and tested under the following environment:

- python = 3.8.2  
- torch = 2.2.0  
- torchvision = 0.17.0  
- numpy = 1.24.0  

ğŸ’¡ Optional: You can install dependencies with pip install -r requirements.txt or manually as shown below.

## ğŸ“ Dataset Setup

The dataset contains **1,463 bridge inspection images**, each annotated with **hierarchical labels** across three levels:  
**structural region â†’ component â†’ defect type**

### ğŸ”½ Download

You can download the full dataset from:

ğŸ”— [Cloud Drive Download](https://ieeexplore.ieee.org/document/10546293) *(access code required)*

> ğŸ“Œ **Note**: The extraction code is not publicly available. Please contact the authors for academic or collaborative use.  
> ğŸ“§ Contact Email: `your_email@example.com`

---

### ğŸ—‚ Directory Structure

After unzipping the dataset to the project root directory, it should have the following structure:

```bash
dataset/
â”œâ”€â”€ Annotations/                 # raw label files for each image (multi-level: structural region/component/defect)
â”œâ”€â”€ files/                       # preprocessed label CSVs
â”‚   â”œâ”€â”€ classification_trainval.csv   # training/validation labels in multi-label format
â”‚   â””â”€â”€ classification_test.csv       # test labels in multi-label format
â”œâ”€â”€ JPEGImages/                  # raw bridge inspection images
â”œâ”€â”€ pool_pkls/                   # region-wise image features (Faster R-CNN)
â”œâ”€â”€ co_occurrence_matrix.pkl     # label co-occurrence matrix (label correlation prior)
â”œâ”€â”€ ent_emb.pkl                  # label embeddings from knowledge graph (used as semantic prior)
â””â”€â”€ T-G-adj.pkl                  # adjacency matrix of textual graph (structural regionâ€“componentâ€“defect hierarchy)
```
---

### âœ… Usage Notes

- All annotations and features are **preprocessed** and ready to use.
- No additional conversion or annotation processing is required.


## ğŸ“¥ Backbone Model: MambaVision-B-1K

The proposed **MKGM** model adopts **[MambaVision-B-1K](https://huggingface.co/nvidia/MambaVision-B-1K)** as the image feature extraction backbone.  
This pretrained model can be easily downloaded and used via [Hugging Face Transformers](https://huggingface.co) or the `timm` library.

```python
from timm import create_model
model = create_model('mambavision_b_1k', pretrained=True)







